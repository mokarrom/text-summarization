# chapter_summarization_api
AWS deployed book chapter summarization (5%) solution 

## Terminology

### MAX_TOKENS
The total number of tokens. This includes token counts in the given text, i.e., book chapter, plus token count in the summary. We are using 90% of `MAX_TOKENS` tokens for the book chapter and the rest 10% for the summary.

### Paragraph
A paragraph is defined as a sequence of nonempty lines (in other words, paragraphs are separated by empty lines).

### Chunk
A chunk consists of a set of paragraphs, where paragraphs are separated by a newline (`\n`). The total number of tokens in a 
chunk must be less than or equal to the `MAX_TOKENS`.

Please note that a paragraph can't be part of two different chunks, whether the whole paragraph should be included or not. 
Therefore, the total number of tokens in a paragraph should be less than or equal to the `MAX_TOKENS`. However, If a 
paragraph is greater than `MAX_TOKENS`, we split that paragraph into sentences and distribute them into different chunks.
If a sentence is greater than `MAX_TOKENS`, we skip that sentence (as we think that is not an ideal sentence).

## Installation
1. Clone the repository
    ```shell
    git clone https://github.com/jasperdew/chapter_summarization_api.git
    ```
2. Create a virtual environment with python ~3.8
    ```shell
    python3 -m venv my_venv 
    ```
3. Activate the `my_venv` virtual environment
    ```shell
    source my_venv/bin/activate  OR   .\my_venv\Scripts\activate
    ```
4. Install all the dependencies into your `my_venv`
    ```shell
    pip install -r requirements.txt
    ```
   
Update `~/src/summarizer/resources/.env` file with your OpenAI API key. **Do not push your API key into GitHub.**

Since we didn't have any package/dependency manager (e.g., poetry) yet, you need to add project 
root directory into the system path. There are differet ways to do that, e.g., modifying sys.path or PYTHONPATH. 
```shell
export PYTHONPATH="${PYTHONPATH}:<your/path>/chapter_summarization_api/src"
```
Now run `~/src/summarizer/scripts/run_chapt_sum.py` script by providing your chapater's text path and ouput summary path as follows:
   ```shell
   python src/summarizer/scripts/run_chapt_sum.py --chapter_dir=<your/chapter/dir> --summary_dir=<your/summary/dir>
   ```
## Running pytest
```shell
 pytest tests/
```

## Summarization
For a given book chapter, we split the text into a set of chunks, summarize those chunks individually to generate chunk summaries, and 
finally, we combine chunk summaries to obtain the chapter summary. 

## Evaluation
To measures the quality of summaries generated by our system we use ROUGE (**R**ecall-**O**riented **U**nderstudy for **G**isting **E**valuation) 
and BLEU (Bilingual Evaluation Understudy) scores. It works by comparing a system-generated summary to a reference summary. We calculate the following ROUGE 
and BLEU metrics:

### ROUGE
We compute **precision** (refers to how much of the system summary was in fact relevant or needed), 
**recall** (refers to how much of the reference summary the system summary is recovering or capturing) 
and **f1-score** (refers system's accuracy, calculated as the harmonic mean of _precision_ and _recall_) to measure the overlap between a system summary and a reference summary.
- **ROUGE-1**: It refers to the overlap of unigrams between the system summary and reference summary.
- **ROUGE-2**: It refers to the overlap of bigrams between the system and reference summary.
- **ROUGE-L**: It refers to the overlap of longest matching sequence of words using LCS.

### BLEU
- **BLEU-1**: It refers BLEU score only for 1-gram matches. This function is implemented using NLK _sentence_bleu_.
- **BLEU-2**: It refers BLEU score only for 1-gram matches. However, we have modified the reference text here. As our system summary 
is measly smaller than the reference summary, we have split reference summary into sentences and have considered each 
sentence as a reference.

## Test Dataset
To assess the system performance we develop a test set based on [booksum](https://github.com/salesforce/booksum) test set.
To get a better intuition about the system response, we further divided booksum test set into three subsets:
- _**booksum-test_chapt_unique-sum.jsonl**_ It contains those chapters that have only one summary.
- _**booksum-test_chapt_multi-sum.jsonl**_ It contains those chapters that have two or more summaries from multiple sources.
- _**booksum-test_books-sections.jsonl**_ It contains samples that are sections or entire books.

## Benchmark Results
Benchmark results are available [here](https://github.com/jasperdew/chapter_summarization_api/blob/develop/docs/ChapterSumResults.xlsx).
For each sample (_i.e., a pair of text and summary_), we calculate eight metrics (_bleu-1, bleu-2, rouge-1-p, rouge-1-r, rouge-1-f, 
rouge-2-p, rouge-2-r, rouge-2-f, rouge-l-p, rouge-l-r, rouge-l-f_) individually and average them to get an average metric on the entire dataset.
From our primary analysis, it appears that **_bleu-2_** and **_rouge-l-f_** are more intuitive (for a glance) than others. 
The overall current metrics are:
```shell
{
    "bleu-1": 0.182,
    "bleu-2": 0.33,
    "rouge-1": {
        "f": 0.239,
        "p": 0.376,
        "r": 0.196
    },
    "rouge-2": {
        "f": 0.045,
        "p": 0.077,
        "r": 0.037
    },
    "rouge-l": {
        "f": 0.216,
        "p": 0.342,
        "r": 0.178
    }
}
```